= kafka-delta-ingest

This is a customized fork of link:https://github.com/delta-io/kafka-delta-ingest[kafka-delta-ingest] tailored to the needs of the Miridih Data Engineering team.

The kafka-delta-ingest project aims to build a highly efficient daemon for
streaming data through link:https://kafka.apache.org[Apache Kafka] into
link:https://delta.io[Delta Lake].

This project is currently in production in a number of organizations and is
still actively evolving in tandem with the
link:https://github.com/delta-io/delta-rs[delta-rs] bindings.

To contribute please look at the link:https://github.com/delta-io/kafka-delta-ingest/blob/main/doc/HACKING.adoc[hacking document].

== Features

* Multiple worker processes per stream
* Basic transformations within message
* Statsd metric output

See the link:https://github.com/delta-io/kafka-delta-ingest/blob/main/doc/DESIGN.md[design doc] for more details.

=== Example

The repository includes an example for trying out the application locally with some fake web request data.

The included docker-compose.yml contains link:https://github.com/wurstmeister/kafka-docker/issues[kafka] and link:https://github.com/localstack/localstack[localstack] services you can run `kafka-delta-ingest` against locally.

==== Starting Worker Processes

1. Launch test services - `docker-compose up setup`
1. Download and extract kafka: `curl -L https://dlcdn.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz | tar -xz`     
1. Create kafka topic: `./kafka_2.13-3.9.0/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic web_requests --create --if-not-exists`
1. Ingest test messages into kafka: `./kafka_2.13-3.9.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic web_requests < tests/json/web_requests-100.json`
1. Compile: `cargo build --features s3`,  MacOS: `brew install librdkafka && cargo build --features s3,dynamic-linking`
1. Run kafka-delta-ingest against the web_requests example topic and table (customize arguments as desired):

```bash
export AWS_ENDPOINT_URL=http://0.0.0.0:4566
export AWS_ACCESS_KEY_ID=test
export AWS_SECRET_ACCESS_KEY=test

RUST_LOG=debug cargo run --features s3,dynamic-linking ingest web_requests ./tests/data/web_requests \
  --allowed_latency 60 \
  --app_id web_requests \
  --transform 'date: substr(meta.producer.timestamp, `0`, `10`)' \
  --transform 'meta.kafka.offset: kafka.offset' \
  --transform 'meta.kafka.partition: kafka.partition' \
  --transform 'meta.kafka.topic: kafka.topic' \
  --auto_offset_reset earliest
```

Notes:

* The AWS_* environment variables are for S3 and are required by the delta-rs library.
** Above, AWS_ENDPOINT_URL points to localstack.
* The Kafka broker is assumed to be at localhost:9092, use -k to override.
* To clean data from previous local runs, execute `./bin/clean-example-data.sh`. You'll need to do this if you destroy your Kafka container between runs since your delta log directory will be out of sync with Kafka offsets.

==== Kafka SSL

In case you have Kafka topics secured by SSL client certificates, you can specify these secrets as environment variables.

For the cert chain include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_CERT`.
For the cert private key include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_KEY`.

These will be set as the `ssl.certificate.pem` and `ssl.key.pem` Kafka settings respectively.

Make sure to provide the additional option:

```
-K security.protocol=SSL
```

when invoking the cli command as well.


== Kafka SSL

In case you have Kafka topics secured by SSL client certificates, you can specify these secrets as environment variables.

For the cert chain include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_CERT`.
For the cert private key include the PEM content as an environment variable named `KAFKA_DELTA_INGEST_KEY`.

These will be set as the `ssl.certificate.pem` and `ssl.key.pem` Kafka settings respectively.

Make sure to provide the additional option:

```
-K security.protocol=SSL
```

when invoking the cli command as well.

== Gzip Compressed Messages

kafka-delta-ingest now supports ingestion of gzip-compressed messages. This can be particularly useful when dealing with large volumes of data that benefit from compression.

To enable gzip decompression, use the `--decompress_gzip` flag when starting the ingestion process.

== Writing to S3

When writing to S3, you may experience an error like `source: StorageError { source: S3Generic("dynamodb locking is not enabled") }`.

A locking mechanism is need to prevent unsafe concurrent writes to a delta lake directory, and DynamoDB is an option for this. To use DynamoDB, set the `AWS_S3_LOCKING_PROVIDER` variable to `dynamodb` and create a table named `delta_rs_lock_table` in Dynamo. An example DynamoDB table creation snippet using the aws CLI follows, and should be customized for your environment's needs (e.g. read/write capacity modes):


```bash
aws dynamodb create-table --table-name delta_rs_lock_table \
    --attribute-definitions \
        AttributeName=key,AttributeType=S \
    --key-schema \
        AttributeName=key,KeyType=HASH \
    --provisioned-throughput \
        ReadCapacityUnits=10,WriteCapacityUnits=10
```

== Schema Support
This application has support for both avro and json format via command line arguments. If no format argument is provided, the default behavior is to use json.
The table below indicates what will happen with respect to the provided arguments.

|===
| Argument      | Value |  Result |
| ----------- | ----------- | ----------- |
| <none>      | <none>       | default json behavior |
| --json      | <any string>       | default json behavior |
| --json      | <schema registry url>       |  will connect schema registry to deserialize json |
| --avro   | ""        | expects all messages in avro format |
| --avro      | <path to an avro schema>       | will use the provided avro schema for deserialization |
| --avro   | <schema registry url>        | will connect schema registry to deserialize avro |
|===


For more information, see link:https://github.com/delta-io/delta-rs/tree/dbc2994c5fddfd39fc31a8f9202df74788f59a01/dynamodb_lock[DynamoDB lock].

